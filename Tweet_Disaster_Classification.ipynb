{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tweets Disaster Classification: LSTM, Attention and Transformers <br>\nAuthor: TeYang, Lau<br>\nCreated: 18/2/2020<br>\nLast update: 6/1/2021<br>\n\n<img src = 'https://bn1301files.storage.live.com/y4m-toxx6sX6SL9zvwtvAbEi9xPKLkgI6kdJ0PJ0uWjzQIR5GouWmvWfMBEuppVlUoFh3eZkKSrveb0QWnLNHPfHVwlBx55CtJMcmqurAYyBv-a2d1rSAmBUxU9CYHY7zZ50XIldgPJMkU7o18TcvrbPJatlu7ioKXMNV0qyev-Z1ise-zNPFjcYmbqz52FSyeW?width=5048&height=1838&cropmode=none' width=\"900\">\n\n<br><br>\n\nIn the era of big data, text and sequential data are the most uniquitous, from social media to medical records to speech recordings. As such, **natural language processing** problems are present in most fields and industry, and deep neural networks that can learn and tackle these problems are becoming increasingly important. The purpose of this notebook is to use some of the most common and effective models to tackle a **sentence classification** problem, specifically, to classify whether a tweet is about a disaster or not. This is easy for a human, but a computer will find it difficult as languages contain multiple complexities. A model will thus will have to take into account the sequential nature of the tweet, the meaning and representation of each word in numbers, as well as the importance and contribution of other words in the same sequence, since two words can have completely different meanings in two different contexts. For example, take the word `kill`. Although it might seem to indicate a disaster, what if it was used in a different context, such as when referring to the book 'To kill a mockingbird'? Thus, NLP is not an easy problem for a computer to solve but recent advances has greatly advanced this process.  \n\nThe dataset contains 10,000 tweets that were classified as disaster or non-disaster.\n\n\n## Project Goals\n1. *Explore* using different sequence models **(LSTM, Attention, Transformers)** for NLP sentence classification problem\n2. *Preprocess/Clean* tweets data into appropriate format for inputting into neural network models \n3. *Understand* **word embeddings** and how they are used to represent words as inputs into NLP models\n4. *Engineer* new features from tweets data that can help to improve model classification\n\n\n### What's in this notebook:\n1. [Data Loading and Structure](#Data_loading_structure)\n2. [Exploratory Data Analysis of Tweets](#EDA) <br>\n2.1. [Distribution of Character, Word and Sentence Frequency](#Frequency_Distribution) <br>\n2.2. [Top Most Common Stopwords](#Top_Stopwords) <br>\n2.3. [Top Most Common Punctuations](#Top_Punc) <br>\n2.4. [Top Most Common Words](#Top_Words) <br>\n2.5. [Wordcloud for Hashtags](#Hashtags) <br>\n3. [Meta-Feature Engineering](#Feature_Engineer)\n4. [Text Data Cleaning](#Data_Clean) <br>\n4.1. [Ngrams](#Ngrams) <br>\n4.2. [WordCloud of Most Common Words after Cleaning](#WC_Cleaned)\n5. [Train Validation Data Split](#TrainValSplit)    \n6. [Embedding Layer](#Embedding) <br>\n6.1. [Tokenization](#Tokenization) <br>\n6.2. [Padding](#Padding) <br>\n6.3. [Embedding Matrix â€“ GloVe](#E_Matrix) <br>\n7. [Model Building & Training](#Model_Build) <br>\n7.1. [Long Short-Term Memory (LSTM)](#LSTM) <br>\n7.2. [Bidirectional LSTM with Attention](#Attention) <br>\n7.3. [BERT](#BERT)\n8. [Error Analysis](#Error)\n9. [Testing](#Test)\n10. [Conclusion](#Conclusion)<br><br>"},{"metadata":{},"cell_type":"markdown","source":"<a id='Data_loading_structure'></a>\n# 1. Data Loading and Structure"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\ntweets = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n#tweets = pd.read_csv(r'C:\\Users\\TeYan\\OneDrive\\Work\\Kaggle\\Tweets_Disaster\\Data\\train.csv')\n#tweets = pd.read_csv('/Users/teyang/OneDrive/Work/Kaggle/Tweets_Disaster/Data/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.isnull().sum().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Location has lots of NaN values and would not be a good/useful feature, unless we have a priori knowledge of where a disaster occured. Furthermore, some of them are not in the correct format, so it will be quite time consuming to clean it. \n\nKeyword has NaNs as well, but can be imputed with 'None'."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncolor = [sns.xkcd_rgb['medium blue'], sns.xkcd_rgb['pale red']]\nsns.countplot('target',data = tweets, palette = color)\nplt.gca().set_ylabel('Samples')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='EDA'></a>\n# 2. Exploratory Data Analysis of Tweets\n\n<a id='Frequency_Distribution'></a>\n## 2.1. Distribution of Character, Word and Sentence Frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import nltk\n#nltk.download('punkt')\nfrom nltk import word_tokenize, sent_tokenize\n\n# count number of characters in each tweet\ntweets['char_len'] = tweets.text.str.len()\n\n# count number of words in each tweet\nword_tokens = [len(word_tokenize(tweet)) for tweet in tweets.text]\ntweets['word_len'] = word_tokens\n\n# count number of sentence in each tweet\nsent_tokens = [len(sent_tokenize(tweet)) for tweet in tweets.text]\ntweets['sent_len'] = sent_tokens\n\nplot_cols = ['char_len','word_len','sent_len']\nplot_titles = ['Character Length','Word Length','Sentence Length']\n\nplt.figure(figsize=(20,4))\nfor counter, i in enumerate([0,1,2]):\n    plt.subplot(1,3,counter+1)\n    sns.distplot(tweets[tweets.target == 1][plot_cols[i]], label='Disaster', color=color[1]).set_title(plot_titles[i])\n    sns.distplot(tweets[tweets.target == 0][plot_cols[i]], label='Non-Disaster', color=color[0])\n    plt.legend()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigate the Outliers\n\ntweets[tweets.sent_len > 8]\ntweets[tweets.word_len > 50]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the outliers such as sentence length > 10 consist of a lot of punctuations. I left it unchanged as I feel that a  tweet with a many sentences, which is indicative of many punctuations, suggest that it is not a serious tweet (about a disaster). Of course there might be some instances where a disaster tweet consists of multiple punctuations (e.g. a volvano just erupted!!!!!!!!!!!!) but that is not very frequent.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id='Top_Stopwords'></a>\n## 2.2. Top Most Common Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot most common stopwords\n\n#nltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n\n# Get all the word tokens in dataframe for Disaster and Non-Disaster\ncorpus0 = [] # Non-Disaster\n[corpus0.append(word.lower()) for tweet in tweets[tweets.target == 0].text for word in word_tokenize(tweet)]\ncorpus1 = [] # Disaster\n[corpus1.append(word.lower()) for tweet in tweets[tweets.target == 1].text for word in word_tokenize(tweet)]\n\n# Function for counting top stopwords in a corpus\ndef count_top_stopwords(corpus):\n    stopwords_freq = {}\n    for word in corpus:\n        if word in stop: \n            if word in stopwords_freq:\n                stopwords_freq[word] += 1\n            else:\n                stopwords_freq[word] = 1\n    topwords = sorted(stopwords_freq.items(), key=lambda item: item[1], reverse=True)[:10] # get the top 10 stopwords\n    x,y = zip(*topwords) # get key and values\n    return x,y\n\nx0,y0 = count_top_stopwords(corpus0)\nx1,y1 = count_top_stopwords(corpus1)\n\n# Plot bar plot of top stopwords for each class\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nplt.bar(x0,y0, color=color[0])\nplt.title('Top Stopwords for Non-Disaster Tweets')\nplt.subplot(1,2,2)\nplt.bar(x1,y1, color=color[1])\nplt.title('Top Stopwords for  Disaster Tweets')\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are lots of occurences of stopwords. These should be removed as they do not predict the target."},{"metadata":{},"cell_type":"markdown","source":"<a id='Top_Punc'></a>\n## 2.3. Top Most Common Punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot most common punctuations\n\nfrom string import punctuation\n\n# Get all the punctuations in dataframe for Disaster and Non-Disaster\ncorpus0 = [] # Non-Disaster\n[corpus0.append(c) for tweet in tweets[tweets.target == 0].text for c in tweet]\ncorpus0 = list(filter(lambda x: x in punctuation, corpus0)) # use filter to select only punctuations\ncorpus1 = [] # Disaster\n[corpus1.append(c) for tweet in tweets[tweets.target == 1].text for c in tweet]\ncorpus1 = list(filter(lambda x: x in punctuation, corpus1)) \n\nfrom collections import Counter\nx0,y0 = zip(*Counter(corpus0).most_common())\nx1,y1 = zip(*Counter(corpus1).most_common())\n\n# Plot bar plot of top punctuations for each class\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nplt.bar(x0,y0, color=color[0])\nplt.title('Top Punctuations for Non-Disaster Tweets')\nplt.subplot(1,2,2)\nplt.bar(x1,y1, color=color[1])\nplt.title('Top Punctuations for Disaster Tweets')\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most common punctuation is the slash, which usually comes from a link ('http://t.co/'). URLs should be removed, as well as most punctuations, with the exception of '!?', which signal some kind of intensity or tonality of the tweet.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id='Top_Words'></a>\n## 2.4. Top Most Common Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot most common words\nimport re\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nstop = ENGLISH_STOP_WORDS.union(stop) # combine stop words from different sources\n\n# function for removing url from text\ndef remove_url(txt):\n    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n\n# Get all the word tokens in dataframe for Disaster and Non-Disaster\n# - remove url, tokenize tweet into words, lowercase words\ncorpus0 = [] # Non-Disaster\n[corpus0.append(word.lower()) for tweet in tweets[tweets.target == 0].text for word in word_tokenize(remove_url(tweet))]\ncorpus0 = list(filter(lambda x: x not in stop, corpus0)) # use filter to unselect stopwords\n\ncorpus1 = [] # Disaster\n[corpus1.append(word.lower()) for tweet in tweets[tweets.target == 1].text for word in word_tokenize(remove_url(tweet))]\ncorpus1 = list(filter(lambda x: x not in stop, corpus1)) # use filter to unselect stopwords\n\n# Create df for word counts to use sns plots\na = Counter(corpus0).most_common()\ndf0 = pd.DataFrame(a, columns=['Word','Count'])\n\na = Counter(corpus1).most_common()\ndf1 = pd.DataFrame(a, columns=['Word','Count'])\n\n# Plot for Disaster and Non-Disaster\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nsns.barplot(x='Word',y='Count',data=df0.head(10), color=color[1]).set_title('Most Common Words for Non-Disasters')\nplt.xticks(rotation=45)\nplt.subplot(1,2,2)\nsns.barplot(x='Word',y='Count',data=df1.head(10), color=color[0]).set_title('Most Common Words for Disasters')\nplt.xticks(rotation=45)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Disaster tweets contain more words related to disasters. But still need more cleaning. And what is the word amp? Will need to expand contractions as well such as 'im'."},{"metadata":{},"cell_type":"markdown","source":"<a id='Hashtags'></a>\n## 2.5. Wordcloud for Hashtags"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(word):\n    for p in punctuation: word = word.replace(p, '')\n    return word\n\nfrom wordcloud import WordCloud\n\ndef wc_hash(target):\n    hashtag = [clean(w[1:].lower()) for tweet in tweets[tweets.target == target].text for w in tweet.split() if '#' in w and w[0] == '#']\n    hashtag = ' '.join(hashtag)\n    my_cloud = WordCloud(background_color='white', stopwords=stop).generate(hashtag)\n\n    plt.subplot(1,2,target+1)\n    plt.imshow(my_cloud, interpolation='bilinear') \n    plt.axis(\"off\")\n\nplt.figure(figsize=(15,4))\nwc_hash(0)\nplt.title('Non-Disaster')\nwc_hash(1)\nplt.title('Disaster')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Feature_Engineer'></a>\n# 3. Meta-Feature Engineering\n\nHere, we extract some features from the tweets that might give us some idea about whether it is a disaster or not. The purpose of this is to build a feature-based model and use it as part of an ensemble model to improve the predictions of the sequence model. Although it might not perform well on its own, it can  actually boost the performance when combined with other models.\n\n* polarity - range of \\[-1,1] where 1 denotes positivity and -1 denotes negativity\n* subjectivity - range of \\[0,1] where 1 denotes personal opinions and 0 denotes factual info\n* exclaimation_num - number of exclamation marks in tweet\n* questionmark_num - number of question marks in tweet\n* url_num - number of urls in tweet\n* hash_num - number of hashtags (#) in tweet\n* mention_num - number of mentions (@) in tweet\n* contraction_num - number of contractions (e.g I'm, we're, we've)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\n\n# polarity and subjectivity\ntweets['polarity'] = [TextBlob(tweet).sentiment.polarity for tweet in tweets.text]\ntweets['subjectivity'] = [TextBlob(tweet).sentiment.subjectivity for tweet in tweets.text]\n\n#############################################################################################################################\n# exclaimation and question marks\ntweets['exclaimation_num'] = [tweet.count('!') for tweet in tweets.text]\ntweets['questionmark_num'] = [tweet.count('?') for tweet in tweets.text]\n\n#############################################################################################################################\n# count number of hashtags and mentions\n# Function for counting number of hashtags and mentions\ndef count_url_hashtag_mention(text):\n    urls_num = len(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n    word_tokens = text.split()\n    hash_num = len([word for word in word_tokens if word[0] == '#' and word.count('#') == 1]) # only appears once in front of word \n    mention_num = len([word for word in word_tokens if word[0] == '@' and word.count('@') == 1]) # only appears once in front of word \n    return urls_num, hash_num, mention_num\n\nurl_num, hash_num, mention_num = zip(*[count_url_hashtag_mention(tweet) for tweet in tweets.text])\ntweets = tweets.assign(url_num = url_num, hash_num = hash_num, mention_num = mention_num)\n\n#############################################################################################################################\n# count number of contractions\ncontractions = [\"'t\", \"'re\", \"'s\", \"'d\", \"'ll\", \"'ve\", \"'m\"]\ntweets['contraction_num'] = [sum([tweet.count(cont) for cont in contractions]) for tweet in tweets.text]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Data_Clean'></a>\n# 4. Text Data Cleaning\n\nThis is the most important step of the entire project â€” text preprocessing/cleaning. This cleans the text into a more 'suitable' form as inputs into the NLP models. For example, URLs might make the text difficult to understand and should be removed when necessary. The choice of whether to remove/clean some words or parts-of-speech is an entire process on its own and sometimes this needs to be experimented. Different models are also able to deal with different kinds of parts-of-speech.\n\n* Replace NaNs with 'None'\n* Expand Contractions\n* Remove Emojis\n* Remove URLs\n* Remove Punctuations except '!?' as they convey intensity and tonality of tweet\n* Replace 'amp' with 'and'\n* Word Segmentaion - segment words such as 'iwould' into 'i' and 'would'\n* Lemmatization - reduces inflected words into their root form; verb part-of-speech tag is used here)\n* Ngrams Exploration \n* Remove Stopwords\n* WordCloud of most commmon words (Unigrams)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Replace NaNs with 'None'\ntweets.keyword.fillna('None', inplace=True) \n\n#############################################################################################################################\n## Expand Contractions\n\n# Function for expanding most common contractions https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ndef decontraction(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ntweets.text = [decontraction(tweet) for tweet in tweets.text]\n\n#############################################################################################################################\n## Remove Emojis\n\n# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nprint(remove_emoji(\"OMG there is a volcano eruption!!! ðŸ˜­ðŸ˜±ðŸ˜·\"))\n\ntweets.text = tweets.text.apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#############################################################################################################################\n## Remove URLs\ntweets.text = tweets.text.apply(lambda x: remove_url(x))\n\n#############################################################################################################################\n## Remove Punctuations except '!?'\n\ndef remove_punct(text):\n    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n    table=str.maketrans('','',new_punct)\n    return text.translate(table)\n\ntweets.text = tweets.text.apply(lambda x: remove_punct(x))\n\n#############################################################################################################################\n## Replace amp\ndef replace_amp(text):\n    text = re.sub(r\" amp \", \" and \", text)\n    return text\n\ntweets.text = tweets.text.apply(lambda x: replace_amp(x))\n\n#############################################################################################################################\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word segmentation takes a long time. So I have commented out the code and loaded the data that has already been segmented beforehand."},{"metadata":{"trusted":true},"cell_type":"code","source":"# from wordsegment import load, segment\n# load()\n\n# tweets.text = tweets.text.apply(lambda x: ' '.join(segment(x)))\n\ntweets = pd.read_csv('../input/twitter-logo/tweets_segmented.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lemmatization\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemma(text):\n    words = word_tokenize(text)\n    return ' '.join([lemmatizer.lemmatize(w.lower(), pos='v') for w in words])\n\ntweets.text = tweets.text.apply(lambda x: lemma(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ngrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Ngrams\nfrom nltk.util import ngrams\n\ndef generate_ngrams(text, n):\n    words = word_tokenize(text)\n    return [' '.join(ngram) for ngram in list(get_data(ngrams(words, n))) if not all(w in stop for w in ngram)] # exclude if all are stopwords\n\n\n# in newer versions of python, raising StopIteration exception to end a generator, which is used in ngram, is deprecated\ndef get_data(gen):\n    try:\n        for elem in gen:\n            yield elem\n    except (RuntimeError, StopIteration):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bigrams\n\nbigrams_disaster = tweets[tweets.target==1].text.apply(lambda x: generate_ngrams(x, 2))\nbigrams_ndisaster = tweets[tweets.target==0].text.apply(lambda x: generate_ngrams(x, 2))\n\nbigrams_d_dict = {}\nfor bgs in bigrams_disaster:\n    for bg in bgs:\n        if bg in bigrams_d_dict:\n            bigrams_d_dict[bg] += 1\n        else:\n            bigrams_d_dict[bg] = 1\n\nbigrams_d_df = pd.DataFrame(bigrams_d_dict.items(), columns=['Bigrams','Count'])\n\nbigrams_nd_dict = {}\nfor bgs in bigrams_ndisaster:\n    for bg in bgs:\n        if bg in bigrams_nd_dict:\n            bigrams_nd_dict[bg] += 1\n        else:\n            bigrams_nd_dict[bg] = 1            \n\nbigrams_nd_df = pd.DataFrame(bigrams_nd_dict.items(), columns=['Bigrams','Count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Barplots for bigrams\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nsns.barplot(x='Count',y='Bigrams',data=bigrams_nd_df.sort_values('Count', ascending=False).head(40), color=color[0]).set_title('Most Common Bigrams for Non-Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.subplot(1,2,2)\nsns.barplot(x='Count',y='Bigrams',data=bigrams_d_df.sort_values('Count', ascending=False).head(40), color=color[1]).set_title('Most Common Bigrams for Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Woudcloud for bigrams\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nmy_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(bigrams_nd_dict)\nplt.imshow(my_cloud, interpolation='bilinear')\nplt.axis('off')\n\nplt.subplot(1,2,2)\nmy_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(bigrams_d_dict)\nplt.imshow(my_cloud, interpolation='bilinear')\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trigrams\n\ntrigrams_disaster = tweets[tweets.target==1].text.apply(lambda x: generate_ngrams(x, 3))\ntrigrams_ndisaster = tweets[tweets.target==0].text.apply(lambda x: generate_ngrams(x, 3))\n\ntrigrams_d_dict = {}\nfor tgs in trigrams_disaster:\n    for tg in tgs:\n        if tg in trigrams_d_dict:\n            trigrams_d_dict[tg] += 1\n        else:\n            trigrams_d_dict[tg] = 1\n\ntrigrams_d_df = pd.DataFrame(trigrams_d_dict.items(), columns=['Trigrams','Count'])\n\ntrigrams_nd_dict = {}\nfor tgs in trigrams_ndisaster:\n    for tg in tgs:\n        if tg in trigrams_nd_dict:\n            trigrams_nd_dict[tg] += 1\n        else:\n            trigrams_nd_dict[tg] = 1            \n\ntrigrams_nd_df = pd.DataFrame(trigrams_nd_dict.items(), columns=['Trigrams','Count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Barplots for trigrams\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nsns.barplot(x='Count',y='Trigrams',data=trigrams_nd_df.sort_values('Count', ascending=False).head(40), color=color[0]).set_title('Most Common Trigrams for Non-Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.subplot(1,2,2)\nsns.barplot(x='Count',y='Trigrams',data=trigrams_d_df.sort_values('Count', ascending=False).head(40), color=color[1]).set_title('Most Common Trigrams for Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Remove Stopwords\ndef remove_stopwords(text):\n    word_tokens = word_tokenize(text)\n    return ' '.join([w.lower() for w in word_tokens if not w.lower() in stop])\n\n#tweets_tmp = tweets.copy()\ntweets['text_nostopwords'] = tweets.text.apply(lambda x: remove_stopwords(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='WC_Cleaned'></a>\n## 4.1. WordCloud of Most Common Words after Cleaning\n\nRemoved some words such as 'new', 'like' and 'people' as they are common between both targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot word cloud for most common words after cleaning\n\nfrom PIL import Image\nmask = np.array(Image.open('../input/twitter-logo/Twitter-Logo_white.png'))\nreverse = mask[...,::-1,:]\n\ndef wc_words(target, mask=mask):\n    words = [word.lower() for tweet in tweets[tweets.target == target].text_nostopwords for word in tweet.split()]\n    words = list(filter(lambda w: w != 'like', words))\n    words = list(filter(lambda w: w != 'new', words))\n    words = list(filter(lambda w: w != 'people', words))\n    dict = {}\n    for w in words:\n        if w in dict:\n            dict[w] += 1\n        else:\n            dict[w] = 1\n    # plot using frequencies        \n    my_cloud = WordCloud(background_color='white', stopwords=stop, mask=mask, random_state=0).generate_from_frequencies(dict) \n    \n    plt.subplot(1,2,target+1)\n    plt.imshow(my_cloud, interpolation='bilinear') \n    plt.axis(\"off\")\n\nplt.figure(figsize=(15,10))\nwc_words(0)\nplt.title('Non-Disaster')\nwc_words(1, reverse)\nplt.title('Disaster')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_colwidth = 200\nfor t in tweets['text'].sample(n=20, random_state=0):\n    print(t)\npd.reset_option('max_colwidth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.reset_option('max_colwidth')\ntweets.drop('text_nostopwords', axis=1, inplace=True)\ntweets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='TrainValSplit'></a>\n# 5. Train Validation Data Split\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(tweets.drop(['id','keyword','location','target'],axis=1), tweets[['target']], test_size=0.2, stratify=tweets[['target']], random_state=0)\nX_train_text = X_train['text']\nX_val_text = X_val['text']\n\nprint('X_train shape: ', X_train.shape)\nprint('X_val shape: ', X_val.shape)\nprint('y_train shape: ', y_train.shape)\nprint('y_val shape: ', y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Class Proportion:\\n', y_train['target'].value_counts() / len(y_train) * 100)\nprint('\\nValidation Class Proportion:\\n', y_val['target'].value_counts() / len(y_val) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Embedding'></a>\n# 6. Embedding Layer\n\n### Word Representation\n\nWord representation refers to representing words as numbers so that a computer can understand it. One way to represent words is to use a one-hot representation (bottom left), where each word in a corpus/dictionary is a vector of all 0s except the index which it is assigned to. For example, in a 10,000 word dictionary, `a` is usually the first word and so is given a vector of [1,0,0,0,...,0], `aaron` is a vector of [0,1,0,0,...,0] and `zulu`, which might be the last word, is a vector of [0,0,0,0,...,1], all with a shape of (10000, 1). However, this way of representing words have a major weakness â€” ***it treats each word as onto itself, so it does not generalize across words.*** For example, the relationship between `apple` and `orange` is not any closer than the relationship between `apple` and `king`. The inner product or Euclidean distance between any 2 words will be 0. Therefore, all word pairs will have a dissimilarity (Euclidean Distance) of 0.\n<br><br>\n\n<img src = 'https://bn1301files.storage.live.com/y4mS1q2-u6bjSL9LZ317bVz57HUlCnt3l3du9-iVCE8GiUrMMM4YAuxWQ12iHTImvYXvnLJgCKWZFE7kiurFmRX7jMUINieWGPGLeP9rtszv3GlaEwvhWiDXo3wfS7tC-semwXswn3QOlKZi1Ddsz9VRS9YABa_6lugTftLC_ZLOfv77igv55y_E_3Lq5AgqFus?width=3676&height=1378&cropmode=none' width=800> \n<br>\n\n### Word Embeddings\n\nA better way to represent words is using word embeddings, which can be learned from large corpuses of texts, such as Wikipedia. It is a dense way, compared to the sparse way for word representation, of representing words as well as the relationships between them. A word embedding is a learned representation for text where words that have the **same meaning have a similar representation**. For example, as shown above in the right table, `apple` and `orange` have similar vector values (their euclidean distance is very small) compared to `apple` and `king`. Another way to compare two words is using **cosine similarity**. \n\nEach row of the matrix represents a **feature/dimension**, such as `gender` or `food` that are attributes of the words. Words that are highly attributed to the feature are given high positive and negative values, while words with no such attributes are given values close to 0s. If we take the vector difference between `man` and `woman`, or `king` and `queen`, both will give a vector close to [-2,0,0,...,0], indicating that each of the pair of words differ highly according to the `gender` attribute. In practice, the features/dimensions that are learned for word embeddings are more abstract, and sometimes it might not be intuitive as to what attributes they represent, and they might be a combination of different attributes.\n\nWord embeddings can be trained from scratch. Some of the most popular ways include [Word2Vec](http://jalammar.github.io/illustrated-word2vec/), [NegativeSampling](http://jalammar.github.io/illustrated-word2vec/), and [GloVe (Global vectors for word representation)](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010). A pre-trained word embedding can also be downloaded and used.\n\nThe graph below shows a simple RNN model for a *many-to-one* classification problem, such as tweet disaster classification or sentiment analysis, with the input words fed into the embedding layer. For each word, its vector representation (`e`) is obtained from the embedding matrix (`E`), and is then fed into the hidden layers.\n<br><br>\n\n<img align=left src = 'https://bn1301files.storage.live.com/y4mkNjcljwqSA1Sb6vCIb8YMk8i5mcl-ViArevkMz6kqZVvbi8fW0lJFPwAprRt5DBN3YamG_ooLd_dRT85rIEinIHrPUTcdxLeBHuxLAYmfpxdDT6Hajvhrqmevt1C_XtXMWQnEe1z2-fouUj760K41kfVH2vbzBOr8JNZYWCNte-xVWHuBSFxGCrzTM7bumTs?width=3368&height=1448&cropmode=none' width=800> \n\n<br>\n"},{"metadata":{},"cell_type":"markdown","source":"<a id='Tokenization'></a>\n## 6.1. Tokenization"},{"metadata":{},"cell_type":"markdown","source":"To feed the tweets into the model, first we need to split them up. Here we **tokenize** the sentences -- break them up into words and assign them an integer based on the vocabulary dictionary. The maximum vocabulary size is set to 5000, so only the most common `num_words`-1 words will be kept. `oov_token` is set to `<UNK>` so that out-of-vocabulary words will be given an index instead of being ignored during `text_to_sequence` call.\n\nUse `fit_on_texts` to create a word-to-index vocabulary dictionary based on the train texts. This creates the vocabulary index based on word frequency, with words that appear more often at the top of the vocabulary.\n\n`texts_to_sequences` transforms each text in texts to a sequence of integers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ntokenizer_1 = Tokenizer(num_words=5000, oov_token='<UNK>')\ntokenizer_1.fit_on_texts(X_train_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_text = tokenizer_1.texts_to_sequences(X_train_text)\nX_val_text = tokenizer_1.texts_to_sequences(X_val_text)\nprint(X_train_text[:10])\nprint('')\nprint(X_val_text[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each list in the `X_train_text` and `X_val_text` is a list of integers, which corresponds to each tweets in the train and validation set respectively. The length of each list is also different as different tweets have different lengths. Therefore, we will need to apply **padding** to make all sequences the same length.\n\nWe can use `tokenizer.word_index` to look at the vocabulary dictionary and `sequences_to_texts` to transform sequences back into texts. Note that words that are not in the vocabulary are now `<UNK>`.\n\n\n**Note:** The Tokenizer stores everything in the `word_index` during `fit_on_texts`. Then, when calling the `texts_to_sequences` method, only the top `num_words` are considered. So `word_index` will actually contain more words than `num_words`."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_1.sequences_to_texts([X_train_text[1]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Padding'></a>\n## 6.2. Padding\n\nAfter tokenization, each tweet is represented as a list of tokens. Next, we need to **pad** all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths). Do this by adding 0s to the end of each sentence in the tokenized form so that each sentence is *now the same length as the longest tweet*. \n\nThe max length for the train set tweets is 32. We will set the `maxlen` to be 50 as tweets from the validation or test set might be longer. This means that texts longer than 50 words will be truncated to the 1st 50 words while texts shorter than 50 will have 0s appended to make them of length 50.\n\nBelow shows a quick example of padding sentences to a length of 5 sequences.\n\n<br>\n\n<img src = 'https://bn1301files.storage.live.com/y4ma9N0t0Cjf_JcFdIj5J6W47lKDiMsXBwUwg5KXo6hUlH9PrpNv5b067TNxP7NFrtk1nbM8fxn5HXFs4rOLJ1QZK1omFFHB5Bl-jsoX5T4bZKJ3I76JwZazSPvquBb0aVem8MGLIP2CT8AsnRW1EOeMExc4w1AkzmfJ_p1oNRv506yRZEUEVlbtY780CnoAadD?width=4342&height=494&cropmode=none' width=700 align=left>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Set Max Length:', max(len(text) for text in X_train_text))\nmaxlen = 50\n\nX_train_text = pad_sequences(X_train_text, padding='post', maxlen=maxlen)\nX_val_text = pad_sequences(X_val_text, padding='post', maxlen=maxlen)\n\nprint('X_train shape:', X_train_text.shape)\nprint('X_train shape:', X_val_text.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='E_Matrix'></a>\n## 6.3. Embedding Matrix â€“ GloVe\n\nWe will use the [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) that were pre-trained on 2 billion tweets to create our feature matrix. First, we will create a dictionary that will contain words as keys and their corresponding embedding list at values. The length of the embedding for each word will be 200, as the GloVe embedding we are using was trained to have 200 dimensions. Refer to [here](https://github.com/stanfordnlp/GloVe) also for more details.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding 1 because of reserved 0 index\nvocab_size = len(tokenizer_1.word_index) + 1\n\n# load the whole embedding into memory\nembeddings_index = dict()\nf = open('../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt')\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will create an embedding matrix for our train vocab/corpus where each row number will correspond to the index of the word in our train vocab/corpus. The matrix will have 200 columns, each containing one of the GloVe feature/dimension."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a weight matrix for words in training set\nembedding_matrix = np.zeros((vocab_size, 200))\n\nfor word, i in tokenizer_1.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        \nprint('Embedding Matrix Shape:', embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Model_Build'></a>\n# 7. Model Building & Training\n\n<a id='LSTM'></a>\n## 7.1. Long Short-Term Memory (LSTM)\n\n[Long Short-Term Memory (LSTM)](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735) models are a type of recurrent neural network that allows for longer range dependencies, unlike traditional feed-forward RNNs. It has a few advantages:\n\n1. Longer range dependence\n2. Selectively remember or forget things\n3. Get around exploding and vanishing gradients\n\nLSTMs have a few dependencies. Imagine that we are predicting whether it will rain today. This depends on several information:\n\n1. The trend of the previous few days, such as many days with rain, or a very heavy downpour (the previous cell state)\n2. Information from the previous day, such as temperature, wind level (previous hidden state)\n3. Information from today (input at current time step)\n\nTo decide whether or not to use these information, LSTMs contain different memory blocks called **cells**, which are responsible for remembering which information are important to use and which to discard. Manipulations or changes to this memory is done through **gates**. \n\n1. The **forget gate** is responsible for removing memories from the cell state. From the figure and equations below, it takes inputs from the previous hidden state, `a` and current input `x`, and applies a sigmoid function to decide whether to keep the information or not. \n2. The **input gate** consists of the **update gate** and the **tanh** function. The process of adding new information to the memory cell is done through this gate. The **update gate** decides which information to be added through a sigmoid function (similar to the forget gate), and the tanh function creates the information to be added. The two outputs are then multiplied together and added to the memory cell.\n3. The **output gate** selects useful information from the current cell state and outputs it. First, it creates a vector after applying **tanh** to the memory cell, then makes a filter using sigmoid function to regulate the information that needs to be used from the previous vector, and multiply them together, thus creating the output and also the hidden state to the next cell.\n\n\n\n<img src='https://bn1301files.storage.live.com/y4moAJV3tGM4StMGVxvRmKYHz14V8F5X2aC0T4WaJdO1M_9QAPti5-3hx69bd-KJRsASCCdYErxqDL9PeNoDkFRFCwJnzpnR3e9w24NFJoOCMj3h_7jG90QjADEDje9hXVGM4sg8ltWrcbi2vz8pCLVBYsCTAQchMBn-JRTsX5ArSXY2r8ah54G_SVTJD9oJQOA?width=1711&height=623&cropmode=none' width=1000>"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Hyperparameters\nnum_epochs=15\ndropout=0.2\nrecurrent_dropout=0.2\nlr=0.0005\nbatch_size=128\nclass_weight = {0: y_train['target'].value_counts()[1]/len(y_train), 1: y_train['target'].value_counts()[0]/len(y_train)} ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use dropout and recurrent dropout to add regularization to the model, which can help with overfitting. Regular dropout works in the vertical direction of the RNN, while recurrent dropout masks the connections between the recurrent units (horizontal direction). Refer to this [post](https://stackoverflow.com/questions/44924690/keras-the-difference-between-lstm-dropout-and-lstm-recurrent-dropout) for more information.\n\nA class weight will also be used. Without it, the model makes a lot more false negatives than false positives. The weighting for the minority class (`disaster`) will be given more weighting, meaning that it will be given more contribution  to the loss computation. This is taken as `(total samples-samples of class) / total samples`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, GlobalMaxPooling1D, LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint\n\nlstm_model = Sequential()\nembedding_layer = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=maxlen, trainable=False)\nlstm_model.add(embedding_layer)\nlstm_model.add(LSTM(128, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)) # try adding dropout later\nlstm_model.add(LSTM(128))\n\n#model.add(Flatten())\nlstm_model.add(Dense(1, activation='sigmoid'))\n\nadam = optimizers.Adam(lr=lr)\nlstm_model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\nprint(lstm_model.summary())\n\n# best hyperparameters\n# num_epochs=15\n# dropout=0.2\n# recurrent_dropout=0.2\n# lr=0.0005\n# batch_size=128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_model_performance(history):   \n    plt.figure(figsize=(15,5))\n    plt.plot(range(num_epochs), history.history['acc'],'-o',\n             label='Train ACC',color='#ff7f0e')\n    plt.plot(range(num_epochs),history.history['val_acc'],'-o',\n             label='Val ACC',color='#1f77b4')\n    x = np.argmax( history.history['val_acc'] ); y = np.max( history.history['val_acc'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#1f77b4')\n    plt.text(x-0.03*xdist,y-0.13*ydist,'max acc\\n%.2f'%y,size=14)\n    plt.ylabel('Accuracy',size=14); plt.xlabel('Epoch',size=14)\n    plt.legend(loc=(0.01,0.75))\n\n    plt2 = plt.gca().twinx()\n    plt2.plot(range(num_epochs),history.history['loss'],'-o',\n              label='Train Loss',color='#2ca02c')\n    plt2.plot(range(num_epochs),history.history['val_loss'],'-o',\n              label='Val Loss',color='#d62728')\n    x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#d62728')\n    plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n   # plt.ylim([-0.2, 2])\n    plt.ylabel('Loss',size=14)\n    plt.xticks(ticks=list(range(num_epochs)),labels=list(range(1, num_epochs+1)))\n    plt.legend(loc='lower left', bbox_to_anchor=(0.01, 0.1))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('lstm_model.h5', monitor='val_acc', save_best_only=True)\nhistory = lstm_model.fit(X_train_text, y_train, batch_size=batch_size, callbacks=[checkpoint], epochs=num_epochs, \n                         class_weight=class_weight, validation_data=(X_val_text, y_val), verbose=1)\nplot_model_performance(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One thing to note is that when using **class weights** for **class imbalance**, the validation loss is consistently higher than the train loss, but this doesn't happen when `class_weight` is turned off. I am not sure what is happening here. Please let me know if anyone has any ideas!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.models import Sequential\n# from keras.layers.core import Activation, Dropout, Dense\n# from keras.layers import Flatten, GlobalMaxPooling1D, LSTM, Bidirectional\n# from keras.layers.embeddings import Embedding\n# from keras import optimizers\n\n# model = Sequential()\n# embedding_layer = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n# model.add(embedding_layer)\n# model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))) # try adding dropout later\n# model.add(Bidirectional(LSTM(128)))\n\n# #model.add(Flatten())\n# model.add(Dense(1, activation='sigmoid'))\n\n# adam = optimizers.Adam(lr=lr)\n# model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\n# print(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Attention'></a>\n## 7.2. Bidirectional LSTM with Attention\n\n<img src = 'https://bn1301files.storage.live.com/y4mpYVhDp9W6iW73-HkwwbyvkDRtQBj8K6FIz4kb7-iQhcydjC0KzXrREYJy-Im10aox7hLJIetYLNhuusOdo6fBkgpSLnnZn2RCf2H-lfqw1CXfsXUv_wFiuf2QAK70HgeNo_Ayl3H4kIbT5FUgCLK0iS21B5uNIAgFXVKapAYwMdMYzmStGqSBkvQ_H4m_9A6?width=850&height=425&cropmode=none' width=800>"},{"metadata":{},"cell_type":"markdown","source":"A vanilla LSTM only uses information from the previous timesteps and not from the future. In many NLP problems, words that come after the current timepoint also influences the current output, although this is less likely for other applications like weather forecasting. As such, a bidirectional LSTM takes into account information from both past and future to create the output at the current timepoint, as shown by the figure above in the LSTM layer. Note that a gated recurrent unit (GRU) can also be used instead of a LSTM.\n\nAlso, another limitation with encoder-decoder architectures is that the encoder has to learn to encode input sequences into a *fixed-length internal representation*, which limits the performance of these networks, especially when considering very long input sequences. This means that the encoder has to compress all the information of a source input into a fixed-length vector and pass it to the encoder. The idea of **attention** aims to search for \"a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.\" â€” [Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)\n\nAs seen from the figure above, the attention layer takes the bidirection hidden layer states and multiply them to a set of attention weights, which tells how much attention the current input should be paying attention to other past and future inputs (the **context**). These outputs at each timepoint will then be concatenated, which is the context, and will be used to generate the output. There are 2 main kinds of attention: **Global** and **Local** Attention.\n* **Global Attention**: Considers all hidden states of encoder LSTM and all hidden states[(Luong et al., 2015)](https://arxiv.org/abs/1508.04025) / previous hidden states [(Bahdanau et al., 2015)](https://arxiv.org/abs/1409.0473) of the unidirectional encoder LSTM. Global attention requires lots of computation as all hidden states are considered.\n* **Local Attention**: Only a part of the encoder hidden states are considered for context vector generation.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Attention Class\n\nfrom keras.layers import Layer\nimport keras.backend as K\n\nclass attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(attention,self).get_config()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Attention\n\n## Hyperparameters\nnum_epochs=15\ndropout=0.3\nrecurrent_dropout=0.3\nlr=0.0005\nbatch_size=128\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras import Model\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, Input, Layer, GlobalMaxPooling1D, LSTM, Bidirectional, Concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras import optimizers\n\n## Embedding Layer\nsequence_input = Input(shape=(maxlen,))\nembedded_sequences = Embedding(vocab_size, 200, weights=[embedding_matrix], trainable=False)(sequence_input)\n\n## RNN Layer\nlstm = Bidirectional(LSTM(128, return_sequences = True, dropout=dropout, recurrent_dropout=recurrent_dropout))(embedded_sequences)\n# Getting our LSTM outputs\n(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(128, return_sequences=True, return_state=True))(lstm)\n\n## Attention Layer\natt_out=attention()(lstm)\noutputs=Dense(1,activation='sigmoid')(att_out)\nmodel_attn = Model(sequence_input, outputs)\n\nadam = optimizers.Adam(lr=lr)\n#sgd = optimizers.sgd(lr=lr)\nmodel_attn.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\n\nprint(model_attn.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('attn_model.h5', monitor='val_acc', save_best_only=True)\nhistory_attn = model_attn.fit(X_train_text, y_train, batch_size=batch_size, callbacks=[checkpoint], epochs=num_epochs, \n                              class_weight=class_weight, validation_data=(X_val_text, y_val), verbose=1)\nplot_model_performance(history_attn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the validation loss is consistently higher than the train loss when using **class weights**."},{"metadata":{},"cell_type":"markdown","source":"<a id='BERT'></a>\n## 7.3. BERT\n\nThe Bidirectional Encoder Representations from Transformers (BERT) is a language model developed by Google which has achieved state-of-the-art results in a variety of NLP tasks. BERT's key innovation is applying bidirectional training (actually it is non-directional, as it reads the entire sequence of words at once) of the encoder part of a **Transformer**.\n\nTo understand BERT, we need to first understand what is a Transformer. The Transformer was first introduced in the very influential paper \"Attention is All You Need\" by [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). Below is the architecture of the Transformer. It gets past the sequential nature of traditional RNNs, and instead considers all inputs at the same time using **multi-headed self-attention**. To get a better intuition of self-attention as well as a more detailed explanation of the Transformer, refer to this [post](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/). Also, since the model is no longer sequential (contains no recurrence), it uses positional encodings to \"inject some information about the relative or absolute position of the tokens in the sequence\". These positional encodings use sine and cosine functions and are added to the input embeddings at the bottom of the encoder and decoder.\n\n<img src = 'https://cdn.analyticsvidhya.com/wp-content/uploads/2019/06/Screenshot-from-2019-06-17-19-53-10.png' align='left'>\n<img src = 'https://cdn.analyticsvidhya.com/wp-content/uploads/2019/06/Screenshot-from-2019-06-17-20-05-30.png'>\n"},{"metadata":{},"cell_type":"markdown","source":"The BERT model uses a multi-layer bidirectional Transformer encoder (stacks the encoder several times). Only the encoder is needed as its goal is to create a language model. It performs self-attention in both directions and is pre-trained using two unsupervised prediction tasks.\n\n**Masked Language Modelling** <br>\n15% of the words in each sequence are masked at random, and the model was trained to predict these masked words based on the context provided by the other non-masked words in the sequence. The loss function only takes into consideration the prediction of the masked values and not the non-masked words.\n\n**Next Sentence Prediction** <br>\nBERT was also pre-trained to capture the relationships between consecutive sentences. It uses pairs of sentences as its training data. 5o% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document while the other half is a random sentence from the corpus.\n\nThe figure below (upper) shows how BERT takes in the input and applies masking and sentence separation. The goal of training BERT is to minimize the combined loss function of these 2 strategies. Refer to this [post](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270) for a more detailed explanation.\n\nFor single sentence classification such as the current problem of classifying disaster tweet, the architecture of BERT will involve adding a classification layer (sigmoid) on top of the Transformer output for the [CLS] token (lower graph below).\n\n<br><br>\n\n<img src = 'https://bn1301files.storage.live.com/y4mGNBqhZEX0ARXkCSvNbAkqw5PNaxxm_STcxiBcYvZVJLhdhjaNWbmnxbhxZwxyhJzPG6B7mjWwCQEWdLKJTtM9e7Z_A2y58uKJi2HoNyaU9wB4y9L66TXdp8UVvUSNPpDJc3XBGEld4gESOXDwZRc4xSYWuG_T7a5t8lDYQ3veqOCeBgt9N3IO6tI_PxaXZJV?width=1425&height=451&cropmode=none' width=600>\n<img src = 'https://media.geeksforgeeks.org/wp-content/uploads/20200422012400/Single-Sentence-Classification-Task.png' width=500>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\nmaxlen = 160\nlr = 1e-5 # 1e-5 \nnum_epochs = 3 # 5\nbatch_size=16 # batch size cannot be too big for bert","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code for building the BERT model was taken from [Wojtek Rosa's notebook](https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data). Credit goes to him for sharing it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tokenization\n\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(bert_layer, max_len=512, lr=1e-5):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(X_train.text.values, tokenizer, max_len=maxlen)\nval_input = bert_encode(X_val.text.values, tokenizer, max_len=maxlen)\ntrain_labels = y_train.target.values\nval_labels = y_val.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model = build_model(bert_layer, max_len=maxlen, lr=lr)\nbert_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('bertmodel.h5', monitor='val_accuracy', save_best_only=True)\n\nbert_history = bert_model.fit(\n    train_input, train_labels,\n    validation_data=(val_input, val_labels),\n    epochs=num_epochs,\n    callbacks=[checkpoint], \n    #class_weight=class_weight,\n    batch_size=batch_size\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RoBERTa\n\nhttps://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb#scrollTo=c3Q9NDdmqEyo"},{"metadata":{},"cell_type":"markdown","source":"<a id='Meta-data'></a>\n## 7.4. Feature-based Model\n\nHere, we will create a feature-based model using the meta-features that we created at the beginning. The idea is to ensemble this model and the sequence models together to get better predictions. When ensembling, the outputs of this model will be given less weight compared to the neural networks as the neural networks are more likely to be better learners."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nclf = RandomForestClassifier(n_estimators=500, max_depth=15, min_samples_split=20, min_samples_leaf=2, n_jobs=-1, random_state=0)\nclf.fit(X_train.drop('text',axis=1), y_train.target.values)\nclf_pred = clf.predict_proba(X_val.drop('text',axis=1))\n\nprint('Validation Accuracy:', accuracy_score(y_val.target.values, clf_pred.argmax(axis=-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_pred.max(axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_pred.max(axis=-1)*0.1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Error'></a>\n# 8. Error Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# val = X_val.copy()\n# val = val[['text']]\n# val['target'] = y_val\n# val['pred'] = model.predict(X_val_text)\n# val['pred'] = (val['pred']*0.8) + (clf_pred.max(axis=-1)*0.2)\n# val['pred'] = val['pred'].apply(lambda x: 1 if x >=0.5 else 0)\n# error = val[val['target'] != val['pred']]\n# error.head()\n\nbert_model.load_weights('bertmodel.h5')\nval = X_val.copy()\nval = val[['text']]\nval['target'] = y_val\n# val['pred'] = lstm_model.predict_classes(X_val_text)\nval['pred'] = bert_model.predict(val_input)\nval['pred'] = val['pred'].apply(lambda x: 1 if x >=0.5 else 0)\nerror = val[val['target'] != val['pred']]\nerror.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Plot confusion matrix\ncm  = confusion_matrix(val.target, val.pred)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8),cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Non-Disaster', 'Disaster'], fontsize=16)\nplt.yticks(range(2), ['Non-Disaster', 'Disaster'], fontsize=16)\nplt.xlabel('Predicted Label',fontsize=18)\nplt.ylabel('True Label',fontsize=18)\nplt.show()\n\nprint('Num False Negatives:',sum((val['target'] == 1) & (val['pred'] == 0)))\nprint('Num False Positives:',sum((val['target'] == 0) & (val['pred'] == 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There appears to be more false negatives than false positives from the validation data, meaning that more tweets are being labelled as `not disaster` when in fact they are, even after using `class_weights` to adjust for the imbalance. Perhaps `disaster` tweets can be given even more weighting depending on the goal/purpose of the classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in error[(error['target'] == 1) & (error['pred'] == 0)]['text'].sample(n=20, random_state=0):\n    print(t)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Test'></a>\n# 9. Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count number of characters in each tweet\ntest['char_len'] = test.text.str.len()\n\n# count number of words in each tweet\nword_tokens = [len(word_tokenize(tweet)) for tweet in test.text]\ntest['word_len'] = word_tokens\n\n# count number of sentence in each tweet\nsent_tokens = [len(sent_tokenize(tweet)) for tweet in test.text]\ntest['sent_len'] = sent_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# polarity and subjectivity\ntest['polarity'] = [TextBlob(tweet).sentiment.polarity for tweet in test.text]\ntest['subjectivity'] = [TextBlob(tweet).sentiment.subjectivity for tweet in test.text]\n\n#############################################################################################################################\n# exclaimation and question marks\ntest['exclaimation_num'] = [tweet.count('!') for tweet in test.text]\ntest['questionmark_num'] = [tweet.count('?') for tweet in test.text]\n\n#############################################################################################################################\n# count number of hashtags and mentions\n# Function for counting number of hashtags and mentions\ndef count_url_hashtag_mention(text):\n    urls_num = len(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n    word_tokens = text.split()\n    hash_num = len([word for word in word_tokens if word[0] == '#' and word.count('#') == 1]) # only appears once in front of word \n    mention_num = len([word for word in word_tokens if word[0] == '@' and word.count('@') == 1]) # only appears once in front of word \n    return urls_num, hash_num, mention_num\n\nurl_num, hash_num, mention_num = zip(*[count_url_hashtag_mention(tweet) for tweet in test.text])\ntest = test.assign(url_num = url_num, hash_num = hash_num, mention_num = mention_num)\n\n#############################################################################################################################\n# count number of contractions\ncontractions = [\"'t\", \"'re\", \"'s\", \"'d\", \"'ll\", \"'ve\", \"'m\"]\ntest['contraction_num'] = [sum([tweet.count(cont) for cont in contractions]) for tweet in test.text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Replace NaNs with 'None'\ntest.keyword.fillna('None', inplace=True) \n\n#############################################################################################################################\n## Expand Contractions\n\n# Function for expanding most common contractions https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ndef decontraction(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ntest.text = [decontraction(tweet) for tweet in test.text]\n\n#############################################################################################################################\n## Remove Emojis\n\n# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nprint(remove_emoji(\"OMG there is a volcano eruption!!! ðŸ˜­ðŸ˜±ðŸ˜·\"))\n\ntest.text = test.text.apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#############################################################################################################################\n## Remove URLs\ntest.text = test.text.apply(lambda x: remove_url(x))\n\n#############################################################################################################################\n## Remove Punctuations except '!?'\n\ndef remove_punct(text):\n    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n    table=str.maketrans('','',new_punct)\n    return text.translate(table)\n\ntest.text = test.text.apply(lambda x: remove_punct(x))\n\n#############################################################################################################################\n## Replace amp\ndef replace_amp(text):\n    text = re.sub(r\" amp \", \" and \", text)\n    return text\n\ntest.text = test.text.apply(lambda x: replace_amp(x))\n\n#############################################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from wordsegment import load, segment\n# load()\n\n# test.text = test.text.apply(lambda x: ' '.join(segment(x)))\n\ntest = pd.read_csv('../input/twitter-logo/tweets_test_segmented.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lemmatization\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemma(text):\n    words = word_tokenize(text)\n    return ' '.join([lemmatizer.lemmatize(w.lower(), pos='v') for w in words])\n\ntest.text = test.text.apply(lambda x: lemma(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenize\ntest_text = test['text']\ntest_text = tokenizer_1.texts_to_sequences(test_text)\n\n# padding\ntest_text = pad_sequences(test_text, padding='post', maxlen=50)\n\nprint('X_test shape:', test_text.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lstm prediction\n# model.predict(test_text)\nlstm_model.load_weights('lstm_model.h5')\nsubmission = test.copy()[['id']]\nsubmission['target'] = lstm_model.predict_classes(test_text)\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission.head())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bi-lstm attention prediction\nmodel_attn.load_weights('attn_model.h5')\nsubmission_attn = test.copy()[['id']]\nsubmission_attn['target'] = model_attn.predict(test_text)\nsubmission_attn['target'] = submission_attn['target'].apply(lambda x: 1 if x >=0.5 else 0)\nsubmission_attn.to_csv('submission_attn.csv', index=False)\ndisplay(submission_attn.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert prediction\n\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\n\nbert_model.load_weights('bertmodel.h5')\nsubmission_bert = test.copy()[['id']]\nsubmission_bert['target'] = bert_model.predict(test_input)\nsubmission_bert['target'] = submission_bert['target'].apply(lambda x: 1 if x >=0.5 else 0)\nsubmission_bert.to_csv('submission_bert.csv', index=False)\ndisplay(submission_bert.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert + meta-features prediction\n\nclf_testpred = clf.predict_proba(test.drop(['id','keyword','location','text'],axis=1))\nsubmission_bert = test.copy()[['id']]\nsubmission_bert['target'] = (bert_model.predict(test_input)*0.8).ravel() + (clf_testpred.max(axis=1)*0.2)\nsubmission_bert['target'] = submission_bert['target'].apply(lambda x: 1 if x >=0.5 else 0)\nsubmission_bert.to_csv('submission_bert_ensemble.csv', index=False)\ndisplay(submission_bert.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_bert['target'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Conclusion'></a>\n# 10. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/\n\nhttps://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/\n\n### attention\nhttps://matthewmcateer.me/blog/getting-started-with-attention-for-classification/\n"},{"metadata":{},"cell_type":"markdown","source":"##  TO DO\n* Word and Char vectorizer\n* Remove numbers? Convert numbers to words?\n* Unigrams, Bigrams and Trigrams\n* Glove; remove stopwords, clean before glove?\n* Logistic Regression, BOW, TD IDF, GloVe, BERT?\n* Check Duplicates\n* Decaying LR"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}